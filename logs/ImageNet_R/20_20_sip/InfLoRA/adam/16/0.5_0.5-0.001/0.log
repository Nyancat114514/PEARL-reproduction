2024-03-30 12:07:48,883 [trainer.py] => config: configs/mimg10_inflora.json
2024-03-30 12:07:48,883 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-30 12:07:48,883 [trainer.py] => lamb: 0.5
2024-03-30 12:07:48,883 [trainer.py] => lame: 0.5
2024-03-30 12:07:48,883 [trainer.py] => prefix: reproduce
2024-03-30 12:07:48,883 [trainer.py] => dataset: ImageNet_R
2024-03-30 12:07:48,883 [trainer.py] => data_path: data/cifar100
2024-03-30 12:07:48,883 [trainer.py] => memory_size: 0
2024-03-30 12:07:48,883 [trainer.py] => memory_per_class: 0
2024-03-30 12:07:48,883 [trainer.py] => fixed_memory: True
2024-03-30 12:07:48,883 [trainer.py] => shuffle: False
2024-03-30 12:07:48,883 [trainer.py] => init_cls: 20
2024-03-30 12:07:48,883 [trainer.py] => increment: 20
2024-03-30 12:07:48,883 [trainer.py] => model_name: InfLoRA
2024-03-30 12:07:48,883 [trainer.py] => net_type: sip
2024-03-30 12:07:48,883 [trainer.py] => embd_dim: 768
2024-03-30 12:07:48,883 [trainer.py] => num_heads: 12
2024-03-30 12:07:48,883 [trainer.py] => total_sessions: 10
2024-03-30 12:07:48,883 [trainer.py] => seed: 0
2024-03-30 12:07:48,883 [trainer.py] => EPSILON: 1e-08
2024-03-30 12:07:48,883 [trainer.py] => init_epoch: 50
2024-03-30 12:07:48,883 [trainer.py] => optim: adam
2024-03-30 12:07:48,883 [trainer.py] => init_lr: 0.001
2024-03-30 12:07:48,883 [trainer.py] => init_lr_decay: 0.1
2024-03-30 12:07:48,883 [trainer.py] => init_weight_decay: 0.0
2024-03-30 12:07:48,884 [trainer.py] => epochs: 50
2024-03-30 12:07:48,884 [trainer.py] => lrate: 0.001
2024-03-30 12:07:48,884 [trainer.py] => lrate_decay: 0.1
2024-03-30 12:07:48,884 [trainer.py] => batch_size: 128
2024-03-30 12:07:48,884 [trainer.py] => weight_decay: 0.0
2024-03-30 12:07:48,884 [trainer.py] => rank: 16
2024-03-30 12:07:48,884 [trainer.py] => num_workers: 16
2024-03-30 12:08:50,458 [trainer.py] => config: configs/mimg10_inflora.json
2024-03-30 12:08:50,458 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-30 12:08:50,458 [trainer.py] => lamb: 0.5
2024-03-30 12:08:50,458 [trainer.py] => lame: 0.5
2024-03-30 12:08:50,458 [trainer.py] => prefix: reproduce
2024-03-30 12:08:50,458 [trainer.py] => dataset: ImageNet_R
2024-03-30 12:08:50,458 [trainer.py] => data_path: data/imagenet_r
2024-03-30 12:08:50,458 [trainer.py] => memory_size: 0
2024-03-30 12:08:50,458 [trainer.py] => memory_per_class: 0
2024-03-30 12:08:50,458 [trainer.py] => fixed_memory: True
2024-03-30 12:08:50,458 [trainer.py] => shuffle: False
2024-03-30 12:08:50,459 [trainer.py] => init_cls: 20
2024-03-30 12:08:50,459 [trainer.py] => increment: 20
2024-03-30 12:08:50,459 [trainer.py] => model_name: InfLoRA
2024-03-30 12:08:50,459 [trainer.py] => net_type: sip
2024-03-30 12:08:50,459 [trainer.py] => embd_dim: 768
2024-03-30 12:08:50,459 [trainer.py] => num_heads: 12
2024-03-30 12:08:50,459 [trainer.py] => total_sessions: 10
2024-03-30 12:08:50,459 [trainer.py] => seed: 0
2024-03-30 12:08:50,459 [trainer.py] => EPSILON: 1e-08
2024-03-30 12:08:50,459 [trainer.py] => init_epoch: 50
2024-03-30 12:08:50,459 [trainer.py] => optim: adam
2024-03-30 12:08:50,459 [trainer.py] => init_lr: 0.001
2024-03-30 12:08:50,459 [trainer.py] => init_lr_decay: 0.1
2024-03-30 12:08:50,459 [trainer.py] => init_weight_decay: 0.0
2024-03-30 12:08:50,459 [trainer.py] => epochs: 50
2024-03-30 12:08:50,459 [trainer.py] => lrate: 0.001
2024-03-30 12:08:50,459 [trainer.py] => lrate_decay: 0.1
2024-03-30 12:08:50,459 [trainer.py] => batch_size: 128
2024-03-30 12:08:50,459 [trainer.py] => weight_decay: 0.0
2024-03-30 12:08:50,459 [trainer.py] => rank: 16
2024-03-30 12:08:50,459 [trainer.py] => num_workers: 16
2024-03-30 12:09:08,605 [trainer.py] => config: configs/mimg10_inflora.json
2024-03-30 12:09:08,605 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-30 12:09:08,605 [trainer.py] => lamb: 0.5
2024-03-30 12:09:08,605 [trainer.py] => lame: 0.5
2024-03-30 12:09:08,605 [trainer.py] => prefix: reproduce
2024-03-30 12:09:08,605 [trainer.py] => dataset: ImageNet_R
2024-03-30 12:09:08,605 [trainer.py] => data_path: data/imagenet_r
2024-03-30 12:09:08,605 [trainer.py] => memory_size: 0
2024-03-30 12:09:08,605 [trainer.py] => memory_per_class: 0
2024-03-30 12:09:08,605 [trainer.py] => fixed_memory: True
2024-03-30 12:09:08,605 [trainer.py] => shuffle: False
2024-03-30 12:09:08,605 [trainer.py] => init_cls: 20
2024-03-30 12:09:08,605 [trainer.py] => increment: 20
2024-03-30 12:09:08,605 [trainer.py] => model_name: InfLoRA
2024-03-30 12:09:08,605 [trainer.py] => net_type: sip
2024-03-30 12:09:08,605 [trainer.py] => embd_dim: 768
2024-03-30 12:09:08,605 [trainer.py] => num_heads: 12
2024-03-30 12:09:08,605 [trainer.py] => total_sessions: 10
2024-03-30 12:09:08,605 [trainer.py] => seed: 0
2024-03-30 12:09:08,605 [trainer.py] => EPSILON: 1e-08
2024-03-30 12:09:08,605 [trainer.py] => init_epoch: 50
2024-03-30 12:09:08,605 [trainer.py] => optim: adam
2024-03-30 12:09:08,605 [trainer.py] => init_lr: 0.001
2024-03-30 12:09:08,605 [trainer.py] => init_lr_decay: 0.1
2024-03-30 12:09:08,605 [trainer.py] => init_weight_decay: 0.0
2024-03-30 12:09:08,605 [trainer.py] => epochs: 50
2024-03-30 12:09:08,606 [trainer.py] => lrate: 0.001
2024-03-30 12:09:08,606 [trainer.py] => lrate_decay: 0.1
2024-03-30 12:09:08,606 [trainer.py] => batch_size: 128
2024-03-30 12:09:08,606 [trainer.py] => weight_decay: 0.0
2024-03-30 12:09:08,606 [trainer.py] => rank: 16
2024-03-30 12:09:08,606 [trainer.py] => num_workers: 16
2024-03-30 12:10:08,645 [trainer.py] => config: configs/mimg10_inflora.json
2024-03-30 12:10:08,646 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-30 12:10:08,646 [trainer.py] => lamb: 0.5
2024-03-30 12:10:08,646 [trainer.py] => lame: 0.5
2024-03-30 12:10:08,646 [trainer.py] => prefix: reproduce
2024-03-30 12:10:08,646 [trainer.py] => dataset: ImageNet_R
2024-03-30 12:10:08,646 [trainer.py] => data_path: data/imagenet-r
2024-03-30 12:10:08,646 [trainer.py] => memory_size: 0
2024-03-30 12:10:08,646 [trainer.py] => memory_per_class: 0
2024-03-30 12:10:08,646 [trainer.py] => fixed_memory: True
2024-03-30 12:10:08,646 [trainer.py] => shuffle: False
2024-03-30 12:10:08,646 [trainer.py] => init_cls: 20
2024-03-30 12:10:08,646 [trainer.py] => increment: 20
2024-03-30 12:10:08,646 [trainer.py] => model_name: InfLoRA
2024-03-30 12:10:08,646 [trainer.py] => net_type: sip
2024-03-30 12:10:08,646 [trainer.py] => embd_dim: 768
2024-03-30 12:10:08,646 [trainer.py] => num_heads: 12
2024-03-30 12:10:08,646 [trainer.py] => total_sessions: 10
2024-03-30 12:10:08,646 [trainer.py] => seed: 0
2024-03-30 12:10:08,646 [trainer.py] => EPSILON: 1e-08
2024-03-30 12:10:08,646 [trainer.py] => init_epoch: 50
2024-03-30 12:10:08,646 [trainer.py] => optim: adam
2024-03-30 12:10:08,646 [trainer.py] => init_lr: 0.001
2024-03-30 12:10:08,646 [trainer.py] => init_lr_decay: 0.1
2024-03-30 12:10:08,646 [trainer.py] => init_weight_decay: 0.0
2024-03-30 12:10:08,646 [trainer.py] => epochs: 50
2024-03-30 12:10:08,646 [trainer.py] => lrate: 0.001
2024-03-30 12:10:08,646 [trainer.py] => lrate_decay: 0.1
2024-03-30 12:10:08,646 [trainer.py] => batch_size: 128
2024-03-30 12:10:08,646 [trainer.py] => weight_decay: 0.0
2024-03-30 12:10:08,646 [trainer.py] => rank: 16
2024-03-30 12:10:08,647 [trainer.py] => num_workers: 16
2024-03-30 12:10:08,735 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
2024-03-30 12:10:24,981 [trainer.py] => config: configs/mimg10_inflora.json
2024-03-30 12:10:24,982 [trainer.py] => device: [device(type='cuda', index=0)]
2024-03-30 12:10:24,982 [trainer.py] => lamb: 0.5
2024-03-30 12:10:24,982 [trainer.py] => lame: 0.5
2024-03-30 12:10:24,982 [trainer.py] => prefix: reproduce
2024-03-30 12:10:24,982 [trainer.py] => dataset: ImageNet_R
2024-03-30 12:10:24,982 [trainer.py] => data_path: data/imagenet-r
2024-03-30 12:10:24,982 [trainer.py] => memory_size: 0
2024-03-30 12:10:24,982 [trainer.py] => memory_per_class: 0
2024-03-30 12:10:24,982 [trainer.py] => fixed_memory: True
2024-03-30 12:10:24,982 [trainer.py] => shuffle: False
2024-03-30 12:10:24,982 [trainer.py] => init_cls: 20
2024-03-30 12:10:24,982 [trainer.py] => increment: 20
2024-03-30 12:10:24,982 [trainer.py] => model_name: InfLoRA
2024-03-30 12:10:24,982 [trainer.py] => net_type: sip
2024-03-30 12:10:24,982 [trainer.py] => embd_dim: 768
2024-03-30 12:10:24,982 [trainer.py] => num_heads: 12
2024-03-30 12:10:24,982 [trainer.py] => total_sessions: 10
2024-03-30 12:10:24,982 [trainer.py] => seed: 0
2024-03-30 12:10:24,982 [trainer.py] => EPSILON: 1e-08
2024-03-30 12:10:24,982 [trainer.py] => init_epoch: 50
2024-03-30 12:10:24,982 [trainer.py] => optim: adam
2024-03-30 12:10:24,982 [trainer.py] => init_lr: 0.001
2024-03-30 12:10:24,982 [trainer.py] => init_lr_decay: 0.1
2024-03-30 12:10:24,982 [trainer.py] => init_weight_decay: 0.0
2024-03-30 12:10:24,982 [trainer.py] => epochs: 50
2024-03-30 12:10:24,982 [trainer.py] => lrate: 0.001
2024-03-30 12:10:24,982 [trainer.py] => lrate_decay: 0.1
2024-03-30 12:10:24,982 [trainer.py] => batch_size: 128
2024-03-30 12:10:24,982 [trainer.py] => weight_decay: 0.0
2024-03-30 12:10:24,982 [trainer.py] => rank: 16
2024-03-30 12:10:24,982 [trainer.py] => num_workers: 16
2024-03-30 12:10:25,070 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
2024-03-30 12:10:27,475 [trainer.py] => All params: 113560291
2024-03-30 12:10:27,478 [trainer.py] => Trainable params: 113560291
2024-03-30 12:10:27,478 [inflora.py] => Learning on 0-20
2024-03-30 12:26:58,899 [inflora.py] => Task 0, Epoch 50/50 => Loss 0.099, Train_accy 97.04
2024-03-30 12:27:11,612 [inflora.py] => Layer 1 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 2 : 2/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 3 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 4 : 3/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 5 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 6 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 7 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 8 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 9 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 10 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 11 : 1/768
2024-03-30 12:27:11,612 [inflora.py] => Layer 12 : 1/768
2024-03-30 12:27:11,627 [trainer.py] => Time:1004.1492192745209
2024-03-30 12:27:17,394 [trainer.py] => Time:5.76654839515686
2024-03-30 12:27:17,394 [inflora.py] => Exemplar size: 0
2024-03-30 12:27:17,394 [trainer.py] => CNN: {'total': 87.68, '00-19': 87.68, 'old': 0, 'new': 87.68}
2024-03-30 12:27:17,394 [trainer.py] => CNN top1 curve: [87.68]
2024-03-30 12:27:17,394 [trainer.py] => CNN top1 with task curve: [87.68]
2024-03-30 12:27:17,394 [trainer.py] => CNN top1 task curve: [1.0]
2024-03-30 12:27:17,401 [trainer.py] => All params: 113560291
2024-03-30 12:27:17,404 [trainer.py] => Trainable params: 310292
2024-03-30 12:27:17,404 [inflora.py] => Learning on 20-40
2024-03-30 12:47:26,117 [inflora.py] => Task 1, Epoch 50/50 => Loss 0.097, Train_accy 97.09
2024-03-30 12:47:46,605 [inflora.py] => Layer 1 : 2/768
2024-03-30 12:47:46,606 [inflora.py] => Layer 2 : 3/768
2024-03-30 12:47:46,606 [inflora.py] => Layer 3 : 2/768
2024-03-30 12:47:46,606 [inflora.py] => Layer 4 : 4/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 5 : 2/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 6 : 2/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 7 : 2/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 8 : 2/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 9 : 1/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 10 : 1/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 11 : 1/768
2024-03-30 12:47:46,607 [inflora.py] => Layer 12 : 1/768
2024-03-30 12:47:46,632 [trainer.py] => Time:1229.2275445461273
2024-03-30 12:47:52,846 [trainer.py] => Time:6.214117527008057
2024-03-30 12:47:52,847 [inflora.py] => Exemplar size: 0
2024-03-30 12:47:52,847 [trainer.py] => CNN: {'total': 79.15, '00-19': 72.92, '20-39': 85.47, 'old': 72.92, 'new': 85.47}
2024-03-30 12:47:52,847 [trainer.py] => CNN top1 curve: [87.68, 79.15]
2024-03-30 12:47:52,847 [trainer.py] => CNN top1 with task curve: [87.68, 86.36]
2024-03-30 12:47:52,847 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745]
2024-03-30 12:47:52,849 [trainer.py] => All params: 113560291
2024-03-30 12:47:52,851 [trainer.py] => Trainable params: 310292
2024-03-30 12:47:52,851 [inflora.py] => Learning on 40-60
2024-03-30 13:00:00,842 [inflora.py] => Task 2, Epoch 50/50 => Loss 0.107, Train_accy 97.01
2024-03-30 13:00:14,443 [inflora.py] => Layer 1 : 2/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 2 : 3/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 3 : 3/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 4 : 4/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 5 : 4/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 6 : 3/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 7 : 3/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 8 : 2/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 9 : 2/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 10 : 2/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 11 : 1/768
2024-03-30 13:00:14,444 [inflora.py] => Layer 12 : 2/768
2024-03-30 13:00:14,474 [trainer.py] => Time:741.6235029697418
2024-03-30 13:00:24,202 [trainer.py] => Time:9.727829933166504
2024-03-30 13:00:24,203 [inflora.py] => Exemplar size: 0
2024-03-30 13:00:24,203 [trainer.py] => CNN: {'total': 70.81, '00-19': 61.75, '20-39': 75.0, '40-59': 78.75, 'old': 68.33, 'new': 78.75}
2024-03-30 13:00:24,203 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81]
2024-03-30 13:00:24,203 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49]
2024-03-30 13:00:24,203 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309]
2024-03-30 13:00:24,206 [trainer.py] => All params: 113560291
2024-03-30 13:00:24,208 [trainer.py] => Trainable params: 310292
2024-03-30 13:00:24,208 [inflora.py] => Learning on 60-80
2024-03-30 13:16:49,586 [inflora.py] => Task 3, Epoch 50/50 => Loss 0.106, Train_accy 96.88
2024-03-30 13:17:04,476 [inflora.py] => Layer 1 : 2/768
2024-03-30 13:17:04,476 [inflora.py] => Layer 2 : 3/768
2024-03-30 13:17:04,476 [inflora.py] => Layer 3 : 4/768
2024-03-30 13:17:04,476 [inflora.py] => Layer 4 : 5/768
2024-03-30 13:17:04,476 [inflora.py] => Layer 5 : 4/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 6 : 3/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 7 : 3/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 8 : 2/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 9 : 3/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 10 : 2/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 11 : 1/768
2024-03-30 13:17:04,477 [inflora.py] => Layer 12 : 2/768
2024-03-30 13:17:04,556 [trainer.py] => Time:1000.3479685783386
2024-03-30 13:17:15,827 [trainer.py] => Time:11.270183324813843
2024-03-30 13:17:15,827 [inflora.py] => Exemplar size: 0
2024-03-30 13:17:15,827 [trainer.py] => CNN: {'total': 67.04, '00-19': 56.45, '20-39': 66.86, '40-59': 63.97, '60-79': 82.77, 'old': 62.18, 'new': 82.77}
2024-03-30 13:17:15,827 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04]
2024-03-30 13:17:15,827 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89]
2024-03-30 13:17:15,827 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569]
2024-03-30 13:17:15,829 [trainer.py] => All params: 113560291
2024-03-30 13:17:15,831 [trainer.py] => Trainable params: 310292
2024-03-30 13:17:15,831 [inflora.py] => Learning on 80-100
2024-03-30 13:36:22,300 [inflora.py] => Task 4, Epoch 50/50 => Loss 0.132, Train_accy 96.17
2024-03-30 13:36:40,916 [inflora.py] => Layer 1 : 2/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 2 : 3/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 3 : 4/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 4 : 5/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 5 : 4/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 6 : 3/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 7 : 3/768
2024-03-30 13:36:40,916 [inflora.py] => Layer 8 : 2/768
2024-03-30 13:36:40,917 [inflora.py] => Layer 9 : 3/768
2024-03-30 13:36:40,917 [inflora.py] => Layer 10 : 2/768
2024-03-30 13:36:40,917 [inflora.py] => Layer 11 : 1/768
2024-03-30 13:36:40,917 [inflora.py] => Layer 12 : 2/768
2024-03-30 13:36:40,950 [trainer.py] => Time:1165.1183171272278
2024-03-30 13:36:56,813 [trainer.py] => Time:15.863054037094116
2024-03-30 13:36:56,813 [inflora.py] => Exemplar size: 0
2024-03-30 13:36:56,813 [trainer.py] => CNN: {'total': 66.9, '00-19': 59.31, '20-39': 64.53, '40-59': 54.73, '60-79': 68.74, '80-99': 82.32, 'old': 62.22, 'new': 82.32}
2024-03-30 13:36:56,813 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9]
2024-03-30 13:36:56,813 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07]
2024-03-30 13:36:56,813 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232]
2024-03-30 13:36:56,815 [trainer.py] => All params: 113560291
2024-03-30 13:36:56,817 [trainer.py] => Trainable params: 310292
2024-03-30 13:36:56,817 [inflora.py] => Learning on 100-120
2024-03-30 13:52:23,515 [inflora.py] => Task 5, Epoch 50/50 => Loss 0.144, Train_accy 96.06
2024-03-30 13:52:40,677 [inflora.py] => Layer 1 : 2/768
2024-03-30 13:52:40,677 [inflora.py] => Layer 2 : 3/768
2024-03-30 13:52:40,677 [inflora.py] => Layer 3 : 4/768
2024-03-30 13:52:40,677 [inflora.py] => Layer 4 : 5/768
2024-03-30 13:52:40,677 [inflora.py] => Layer 5 : 4/768
2024-03-30 13:52:40,677 [inflora.py] => Layer 6 : 3/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 7 : 3/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 8 : 2/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 9 : 3/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 10 : 2/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 11 : 1/768
2024-03-30 13:52:40,678 [inflora.py] => Layer 12 : 2/768
2024-03-30 13:52:40,713 [trainer.py] => Time:943.8964035511017
2024-03-30 13:53:00,114 [trainer.py] => Time:19.40019130706787
2024-03-30 13:53:00,114 [inflora.py] => Exemplar size: 0
2024-03-30 13:53:00,114 [trainer.py] => CNN: {'total': 62.88, '00-19': 52.58, '20-39': 59.88, '40-59': 53.35, '60-79': 61.99, '80-99': 68.92, '100-119': 79.82, 'old': 59.82, 'new': 79.82}
2024-03-30 13:53:00,115 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9, 62.88]
2024-03-30 13:53:00,115 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07, 78.97]
2024-03-30 13:53:00,115 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232, 0.682214948172395]
2024-03-30 13:53:00,120 [trainer.py] => All params: 113560291
2024-03-30 13:53:00,124 [trainer.py] => Trainable params: 310292
2024-03-30 13:53:00,124 [inflora.py] => Learning on 120-140
2024-03-30 14:08:36,935 [inflora.py] => Task 6, Epoch 50/50 => Loss 0.138, Train_accy 95.65
2024-03-30 14:09:02,405 [inflora.py] => Layer 1 : 2/768
2024-03-30 14:09:02,408 [inflora.py] => Layer 2 : 3/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 3 : 4/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 4 : 5/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 5 : 4/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 6 : 3/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 7 : 3/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 8 : 2/768
2024-03-30 14:09:02,409 [inflora.py] => Layer 9 : 4/768
2024-03-30 14:09:02,410 [inflora.py] => Layer 10 : 2/768
2024-03-30 14:09:02,410 [inflora.py] => Layer 11 : 1/768
2024-03-30 14:09:02,410 [inflora.py] => Layer 12 : 2/768
2024-03-30 14:09:02,507 [trainer.py] => Time:962.3828418254852
2024-03-30 14:09:24,171 [trainer.py] => Time:21.65726923942566
2024-03-30 14:09:24,171 [inflora.py] => Exemplar size: 0
2024-03-30 14:09:24,172 [trainer.py] => CNN: {'total': 56.28, '00-19': 49.86, '20-39': 53.34, '40-59': 44.57, '60-79': 54.0, '80-99': 57.32, '100-119': 60.71, '120-139': 72.59, 'old': 53.66, 'new': 72.59}
2024-03-30 14:09:24,172 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9, 62.88, 56.28]
2024-03-30 14:09:24,172 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07, 78.97, 73.6]
2024-03-30 14:09:24,172 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232, 0.682214948172395, 0.6333098426121682]
2024-03-30 14:09:24,174 [trainer.py] => All params: 113560291
2024-03-30 14:09:24,176 [trainer.py] => Trainable params: 310292
2024-03-30 14:09:24,176 [inflora.py] => Learning on 140-160
2024-03-30 14:22:37,671 [inflora.py] => Task 7, Epoch 50/50 => Loss 0.116, Train_accy 96.12
2024-03-30 14:23:05,327 [inflora.py] => Layer 1 : 2/768
2024-03-30 14:23:05,327 [inflora.py] => Layer 2 : 3/768
2024-03-30 14:23:05,327 [inflora.py] => Layer 3 : 4/768
2024-03-30 14:23:05,327 [inflora.py] => Layer 4 : 5/768
2024-03-30 14:23:05,327 [inflora.py] => Layer 5 : 4/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 6 : 3/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 7 : 3/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 8 : 2/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 9 : 4/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 10 : 2/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 11 : 1/768
2024-03-30 14:23:05,328 [inflora.py] => Layer 12 : 2/768
2024-03-30 14:23:05,421 [trainer.py] => Time:821.2454609870911
2024-03-30 14:23:28,635 [trainer.py] => Time:23.213409423828125
2024-03-30 14:23:28,635 [inflora.py] => Exemplar size: 0
2024-03-30 14:23:28,636 [trainer.py] => CNN: {'total': 52.44, '00-19': 43.41, '20-39': 50.15, '40-59': 44.34, '60-79': 46.0, '80-99': 53.31, '100-119': 53.39, '120-139': 61.76, '140-159': 72.1, 'old': 50.48, 'new': 72.1}
2024-03-30 14:23:28,636 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9, 62.88, 56.28, 52.44]
2024-03-30 14:23:28,636 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07, 78.97, 73.6, 70.79]
2024-03-30 14:23:28,636 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232, 0.682214948172395, 0.6333098426121682, 0.5935897435897436]
2024-03-30 14:23:28,641 [trainer.py] => All params: 113560291
2024-03-30 14:23:28,645 [trainer.py] => Trainable params: 310292
2024-03-30 14:23:28,645 [inflora.py] => Learning on 160-180
2024-03-30 14:40:55,534 [inflora.py] => Task 8, Epoch 50/50 => Loss 0.097, Train_accy 97.19
2024-03-30 14:41:10,956 [inflora.py] => Layer 1 : 2/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 2 : 3/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 3 : 4/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 4 : 5/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 5 : 4/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 6 : 3/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 7 : 3/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 8 : 2/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 9 : 4/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 10 : 2/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 11 : 1/768
2024-03-30 14:41:10,958 [inflora.py] => Layer 12 : 2/768
2024-03-30 14:41:11,102 [trainer.py] => Time:1062.4570410251617
2024-03-30 14:41:36,226 [trainer.py] => Time:25.124135732650757
2024-03-30 14:41:36,227 [inflora.py] => Exemplar size: 0
2024-03-30 14:41:36,227 [trainer.py] => CNN: {'total': 53.27, '00-19': 44.27, '20-39': 47.53, '40-59': 44.34, '60-79': 46.18, '80-99': 53.18, '100-119': 54.82, '120-139': 50.59, '140-159': 52.96, '160-179': 83.9, 'old': 49.21, 'new': 83.9}
2024-03-30 14:41:36,227 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9, 62.88, 56.28, 52.44, 53.27]
2024-03-30 14:41:36,227 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07, 78.97, 73.6, 70.79, 73.16]
2024-03-30 14:41:36,227 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232, 0.682214948172395, 0.6333098426121682, 0.5935897435897436, 0.5859271835502735]
2024-03-30 14:41:36,230 [trainer.py] => All params: 113560291
2024-03-30 14:41:36,233 [trainer.py] => Trainable params: 310292
2024-03-30 14:41:36,234 [inflora.py] => Learning on 180-200
2024-03-30 14:58:36,781 [inflora.py] => Task 9, Epoch 50/50 => Loss 0.115, Train_accy 96.79
2024-03-30 14:58:49,867 [inflora.py] => Layer 1 : 2/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 2 : 3/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 3 : 4/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 4 : 5/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 5 : 4/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 6 : 3/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 7 : 3/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 8 : 2/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 9 : 4/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 10 : 2/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 11 : 1/768
2024-03-30 14:58:49,867 [inflora.py] => Layer 12 : 2/768
2024-03-30 14:58:49,899 [trainer.py] => Time:1033.6649723052979
2024-03-30 14:59:03,435 [trainer.py] => Time:13.53579306602478
2024-03-30 14:59:03,435 [inflora.py] => Exemplar size: 0
2024-03-30 14:59:03,435 [trainer.py] => CNN: {'total': 49.28, '00-19': 41.26, '20-39': 39.53, '40-59': 36.49, '60-79': 42.81, '80-99': 47.51, '100-119': 45.18, '120-139': 40.61, '140-159': 41.37, '160-179': 67.79, '180-199': 80.83, 'old': 45.12, 'new': 80.83}
2024-03-30 14:59:03,435 [trainer.py] => CNN top1 curve: [87.68, 79.15, 70.81, 67.04, 66.9, 62.88, 56.28, 52.44, 53.27, 49.28]
2024-03-30 14:59:03,435 [trainer.py] => CNN top1 with task curve: [87.68, 86.36, 79.49, 79.89, 80.07, 78.97, 73.6, 70.79, 73.16, 70.5]
2024-03-30 14:59:03,435 [trainer.py] => CNN top1 task curve: [1.0, 0.8744588744588745, 0.805937328202309, 0.7481108312342569, 0.7376046361880232, 0.682214948172395, 0.6333098426121682, 0.5935897435897436, 0.5859271835502735, 0.5363333333333333]
